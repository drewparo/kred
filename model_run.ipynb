{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "from utils.util import *\n",
    "from train_test import *\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from parse_config import ConfigParser"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_compressed_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "restored_data = load_compressed_pickle('/datasets/mind_data/data_dict_compressed.pickle')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "user_history=restored_data[\"user_history\"]\n",
    "entity_embedding=restored_data[\"entity_embedding\"]\n",
    "relation_embedding=restored_data[\"relation_embedding\"]\n",
    "entity_adj=restored_data[\"entity_adj\"]\n",
    "relation_adj=restored_data[\"relation_adj\"]\n",
    "news_feature=restored_data[\"news_feature\"]\n",
    "max_entity_freq=restored_data[\"max_entity_freq\"]\n",
    "max_entity_pos=restored_data[\"max_entity_pos\"]\n",
    "max_entity_type=restored_data[\"max_entity_type\"]\n",
    "train_data=restored_data[\"train_data\"]\n",
    "dev_data=restored_data[\"dev_data\"]\n",
    "vert_train=restored_data[\"vert_train\"]\n",
    "vert_test=restored_data[\"vert_test\"]\n",
    "pop_train=restored_data[\"pop_train\"]\n",
    "pop_test=restored_data[\"pop_test\"]\n",
    "item2item_train=restored_data[\"item2item_train\"]\n",
    "item2item_test=restored_data[\"item2item_test\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='KRED')\n",
    "parser.add_argument('-f')\n",
    "parser.add_argument('-c', '--config', default=\"./config.json\", type=str,\n",
    "                    help='config file path (default: None)')\n",
    "parser.add_argument('-r', '--resume', default=None, type=str,\n",
    "                    help='path to latest checkpoint (default: None)')\n",
    "parser.add_argument('-d', '--device', default=None, type=str,\n",
    "                    help='indices of GPUs to enable (default: all)')\n",
    "config = ConfigParser.from_args(parser)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "train_type = \"single_task\"\n",
    "task = \"user2item\"\n",
    "\n",
    "config['trainer']['epochs'] = epochs\n",
    "config['data_loader']['batch_size'] = batch_size\n",
    "config['trainer']['training_type'] = train_type\n",
    "config['trainer']['task'] = task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "if config['trainer']['training_type']  == \"multi-task\":\n",
    "    data = user_history, entity_embedding, relation_embedding, entity_adj, relation_adj, news_feature, max_entity_freq, max_entity_pos, max_entity_type, train_data, dev_data, vert_train, vert_test, pop_train, pop_test, item2item_train, item2item_test\n",
    "elif config['trainer']['task'] == \"user2item\":\n",
    "    data = user_history, entity_embedding, relation_embedding, entity_adj, relation_adj, news_feature, max_entity_freq, max_entity_pos, max_entity_type, train_data, dev_data\n",
    "elif config['trainer']['task'] == \"item2item\":\n",
    "    data =  user_history, entity_embedding, relation_embedding, entity_adj, relation_adj, news_feature, max_entity_freq, max_entity_pos, max_entity_type, item2item_train, item2item_test\n",
    "elif config['trainer']['task'] == \"vert_classify\":\n",
    "    data = user_history, entity_embedding, relation_embedding, entity_adj, relation_adj, news_feature, max_entity_freq, max_entity_pos, max_entity_type, vert_train, vert_test\n",
    "elif config['trainer']['task'] == \"pop_predict\":\n",
    "    data = user_history, entity_embedding, relation_embedding, entity_adj, relation_adj, news_feature, max_entity_freq, max_entity_pos, max_entity_type, pop_train, pop_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#Single Task Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "user_history_dict, entity_embedding, relation_embedding, entity_adj, relation_adj, doc_feature_dict, entity_num, position_num, type_num, train_data, test_data = data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "train_data_u2i = NewsDataset(train_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "train_sampler_u2i = RandomSampler(train_data_u2i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "['N35729', 'N0', 'N0', 'N0', 'N55689']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_data['item2'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "train_dataloader_u2i = DataLoader(train_data_u2i, sampler=train_sampler_u2i,\n",
    "                                          batch_size=config['data_loader']['batch_size'],\n",
    "                                          collate_fn=my_collate_fn, pin_memory=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "criterion = Softmax_BCELoss(config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "train_data_loader = train_dataloader_u2i"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "device, deviceids = prepare_device(config['n_gpu'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "[0]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deviceids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "model = KREDModel(config, user_history_dict, doc_feature_dict, entity_embedding, relation_embedding, entity_adj,relation_adj, entity_num, position_num, type_num).cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config['optimizer']['lr'], weight_decay=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "trainer = Trainer(config, model, criterion, optimizer, device, train_data_loader, data[-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "--- starting cleaning --- 3241390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/model/News_embedding.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  entity_num_embedding = self.entity_num_embeddings(torch.tensor(entity_nums).cuda())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3224828) max1\n",
      "tensor(3240578) max12\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Half but found Float",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [23]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/notebooks/trainer/trainer.py:110\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    108\u001B[0m early_stopping \u001B[38;5;241m=\u001B[39m EarlyStopping(patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrainer\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mearly_stop\u001B[39m\u001B[38;5;124m'\u001B[39m], verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart_epoch, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepochs\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m--> 110\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    111\u001B[0m     valid_socre \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_valid_epoch(epoch)\n\u001B[1;32m    112\u001B[0m     valid_scores\u001B[38;5;241m.\u001B[39mappend(valid_socre)\n",
      "File \u001B[0;32m/notebooks/trainer/trainer.py:53\u001B[0m, in \u001B[0;36mTrainer._train_epoch\u001B[0;34m(self, epoch)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_dataloader):\n\u001B[1;32m     52\u001B[0m     batch \u001B[38;5;241m=\u001B[39m real_batch(batch)\n\u001B[0;32m---> 53\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mitem1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mitem2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrainer\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtask\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     54\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(out, torch\u001B[38;5;241m.\u001B[39mFloatTensor(batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mcuda())\n\u001B[1;32m     55\u001B[0m     all_loss \u001B[38;5;241m=\u001B[39m all_loss \u001B[38;5;241m+\u001B[39m loss\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/notebooks/model/KRED.py:63\u001B[0m, in \u001B[0;36mKREDModel.forward\u001B[0;34m(self, user_features, news_features, task)\u001B[0m\n\u001B[1;32m     60\u001B[0m         user_embedding \u001B[38;5;241m=\u001B[39m user_embedding\u001B[38;5;241m.\u001B[39mexpand(candidate_news_embedding\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], user_embedding\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m],\n\u001B[1;32m     61\u001B[0m                                            user_embedding\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m])\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 63\u001B[0m     user_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muser_modeling\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m     candidate_news_embedding, topk_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnews_embedding(news_features)\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(candidate_news_embedding\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlen\u001B[39m(user_embedding\u001B[38;5;241m.\u001B[39mshape):\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/notebooks/model/User_modeling.py:42\u001B[0m, in \u001B[0;36mUser_modeling.forward\u001B[0;34m(self, user_id)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, user_id):\n\u001B[1;32m     41\u001B[0m     user_history \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_user_history(user_id)\n\u001B[0;32m---> 42\u001B[0m     user_history_embedding, top_indexs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnews_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser_history\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m     user_attention_modeling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muser_attention_modeling(user_history_embedding)\n\u001B[1;32m     44\u001B[0m     user_embedding \u001B[38;5;241m=\u001B[39m user_attention_modeling\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/notebooks/model/News_embedding.py:152\u001B[0m, in \u001B[0;36mNews_embedding.forward\u001B[0;34m(self, news_id)\u001B[0m\n\u001B[1;32m    149\u001B[0m istitle_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_title_embedding(istitle)\n\u001B[1;32m    150\u001B[0m type_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_type_embedding(type_)\n\u001B[0;32m--> 152\u001B[0m kgat_entity_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkgat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mentities\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# batch(news num) * entity num\u001B[39;00m\n\u001B[1;32m    153\u001B[0m news_entity_embedding \u001B[38;5;241m=\u001B[39m kgat_entity_embeddings \u001B[38;5;241m+\u001B[39m entity_num_embedding \u001B[38;5;241m+\u001B[39m istitle_embedding \u001B[38;5;241m+\u001B[39m type_embedding \u001B[38;5;66;03m#todo\u001B[39;00m\n\u001B[1;32m    155\u001B[0m aggregate_embedding, topk_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_layer(news_entity_embedding, torch\u001B[38;5;241m.\u001B[39mFloatTensor(context_vecs)\u001B[38;5;241m.\u001B[39mcuda())\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/notebooks/model/KGAT.py:149\u001B[0m, in \u001B[0;36mKGAT.forward\u001B[0;34m(self, entity_ids)\u001B[0m\n\u001B[1;32m    147\u001B[0m embedding_concat \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([entity_embedding_expand, neighbor_entity_embedding, neighbor_relation_embedding], \u001B[38;5;241m4\u001B[39m)\n\u001B[1;32m    148\u001B[0m embedding_concat \u001B[38;5;241m=\u001B[39m embedding_concat\u001B[38;5;241m.\u001B[39mhalf()\n\u001B[0;32m--> 149\u001B[0m attention_value \u001B[38;5;241m=\u001B[39m  \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msoftmax(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_layer2(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention_layer1\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding_concat\u001B[49m\u001B[43m)\u001B[49m)))\n\u001B[1;32m    150\u001B[0m neighbor_att_embedding \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(attention_value \u001B[38;5;241m*\u001B[39m neighbor_entity_embedding, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m    151\u001B[0m kgat_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maggregate(entity_embedding, neighbor_att_embedding)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: expected scalar type Half but found Float"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from model.News_embedding import News_embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "news_232 = News_embedding(config, doc_feature_dict, entity_embedding, relation_embedding, entity_adj, relation_adj, entity_num, position_num, type_num)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "news_id = [['N56613', 'N56613', 'N548', 'N12889', 'N2994', 'N19829', 'N5123', 'N3346', 'N56586', 'N2665', 'N51061', 'N23817', 'N50454', 'N8391', 'N21623', 'N43295', 'N45589']]\n",
    "#'N56613', 'N548', 'N12889', 'N2994', 'N19829', 'N5123', 'N3346', 'N56586', 'N2665', 'N51061', 'N23817', 'N50454', 'N8391', 'N21623', 'N43295', 'N45589'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_ids = news_232.get_entities_ids(news_id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kgat2232 = news_232.kgat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "neighbor_entities, neighbor_relations = kgat2232.get_neighbors(entity_ids)\n",
    "#Removed not present id\n",
    "neighbor_entities_tensor = torch.tensor(neighbor_entities).cuda()\n",
    "filter_mask = (neighbor_entities_tensor < 3241390).cuda()\n",
    "neighbor_entities_tensor[~filter_mask] = 0\n",
    "#Removed not present id\n",
    "entity_ids_tensor = torch.tensor(entity_ids).cuda()\n",
    "filter_mask = (entity_ids_tensor < 3241390).cuda()\n",
    "entity_ids_tensor[~filter_mask] = 0\n",
    "entity_ids_tensor = entity_ids_tensor.cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_embedding_lookup = nn.Embedding.from_pretrained(kgat2232.entity_embedding.cuda())\n",
    "relation_embedding_lookup = nn.Embedding.from_pretrained(kgat2232.relation_embedding.cuda())\n",
    "neighbor_entity_embedding = entity_embedding_lookup(neighbor_entities_tensor.cuda())\n",
    "neighbor_relation_embedding = relation_embedding_lookup(torch.tensor(neighbor_relations).cuda())\n",
    "entity_embedding = entity_embedding_lookup(entity_ids_tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_embedding_expand = torch.unsqueeze(entity_embedding, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_embedding_expand = entity_embedding_expand.expand(entity_embedding_expand.shape[0], entity_embedding_expand.shape[1],entity_embedding_expand.shape[2], kgat2232.config['model']['entity_neighbor_num'], entity_embedding_expand.shape[4])\n",
    "embedding_concat = torch.cat([entity_embedding_expand, neighbor_entity_embedding, neighbor_relation_embedding], 4)\n",
    "kgat2232.softmax(kgat2232.attention_layer2(kgat2232.relu(kgat2232.attention_layer1(embedding_concat))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embedding_concat.get_device()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_embedding_expand = entity_embedding_expand.expand(entity_embedding_expand.shape[0], entity_embedding_expand.shape[1],entity_embedding_expand.shape[2], config['model']['entity_neighbor_num'], entity_embedding_expand.shape[4])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embedding_concat = torch.cat([entity_embedding_expand, neighbor_entity_embedding, neighbor_relation_embedding], 4).cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "attention_value =  kgat2232.softmax(kgat2232.attention_layer2(kgat2232.relu(kgat2232.attention_layer1(embedding_concat))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "neighbor_entities_tensor = torch.tensor(neighbor_entities)\n",
    "filter_mask = torch.tensor(neighbor_entities) < 3241390\n",
    "neighbor_entities_tensor[~filter_mask] = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_embedding = entity_embedding_lookup(entity_ids_tensor)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_ids_tensor = torch.tensor(entities).cuda()\n",
    "filter_mask = (entity_ids_tensor < 3241390).cuda()\n",
    "entity_ids_tensor[~filter_mask] = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "neighbor_entity_embedding = entity_embedding_lookup(neighbor_entities_tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "neighbor_entity_embedding.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entities = [[[172, 0]]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_neighbors(entities):\n",
    "    neighbor_entities = []\n",
    "    neighbor_relations = []\n",
    "    for entity_batch in entities:\n",
    "        neighbor_entities.append([])\n",
    "        neighbor_relations.append([])\n",
    "        for entity in entity_batch:\n",
    "            if type(entity) == int:\n",
    "                neighbor_entities[-1].append(entity_adj_2[entity])\n",
    "                neighbor_relations[-1].append(relation_adj_2[entity])\n",
    "            else:\n",
    "                neighbor_entities[-1].append([])\n",
    "                neighbor_relations[-1].append([])\n",
    "                for entity_i in entity:\n",
    "                    neighbor_entities[-1][-1].append(entity_adj_2[entity_i])\n",
    "                    neighbor_relations[-1][-1].append(relation_adj_2[entity_i])\n",
    "\n",
    "    return neighbor_entities, neighbor_relations\n",
    "\n",
    "tmp_nei = get_neighbors(entities)\n",
    "used_nei = kgat2232.get_neighbors(entities)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "used_nei[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for x in tmp_nei[0][0][0][0]:\n",
    "    if x in used_nei[0][0][0][0]:\n",
    "        print(f'ok {x}')\n",
    "    else:\n",
    "        print(f'x {x}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kgat2232.entity_embedding.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "used_nei[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_entity_embedding(neighbor_entities):\n",
    "    entity_embedding_batch = []\n",
    "    if type(neighbor_entities[0][0]) == int:\n",
    "        neighbor_entities = torch.LongTensor(neighbor_entities)\n",
    "        for i in range(len(neighbor_entities)):\n",
    "            entity_embedding_batch.append([])\n",
    "            for j in range(len(neighbor_entities[i])):\n",
    "                entity_embedding_batch[i].append([])\n",
    "                for entityid in neighbor_entities[i][j]:\n",
    "                    entity_embedding_batch[i][j].append(self.entity_embedding[entityid])\n",
    "    else:\n",
    "        neighbor_entities = torch.LongTensor(neighbor_entities)\n",
    "        for i in range(len(neighbor_entities)):\n",
    "            entity_embedding_batch.append([])\n",
    "            for j in range(len(neighbor_entities[i])):\n",
    "                entity_embedding_batch[i].append([])\n",
    "                for k in range(len(neighbor_entities[i][j])):\n",
    "                    entity_embedding_batch[i][j].append([])\n",
    "                    for entityid in neighbor_entities[i][j][k]:\n",
    "                        entity_embedding_batch[i][j][k].append(self.entity_embedding[entityid])\n",
    "    return torch.FloatTensor(torch.stack(entity_embedding_batch)).cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_embedding_lookup(torch.tensor(neighbor_entities))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kgat2232.entity_embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.clone(entity_embedding).cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.tensor(kgat2232.entity_embedding).cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_embedding_lookup = nn.Embedding.from_pretrained(kgat2232.entity_embedding)\n",
    "relation_embedding_lookup = nn.Embedding.from_pretrained(kgat2232.relation_embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "neighbor_relation_embedding = relation_embedding_lookup(torch.tensor(neighbor_relations))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_embedding = entity_embedding_lookup(torch.tensor(entities))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'{len(entity_nums)}, {len(entity_nums[0])}, {len(entity_nums[0][0])}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_num_embedding = news_232.get_entity_num_embedding(entity_nums)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kgat2232.entity_embedding.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.min(kgat2232.entity_embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.nan_to_num(kgat2232.entity_embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kgat2232.entity_embedding[:2,:].cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.zeros([3000000000,100]).cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_adj"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len([['N20076', 'N45590', 'N13579', 'N47817', 'N17583'], ['N45375', 'N52446', 'N62416', 'N45543', 'N15952'], ['N24686', 'N61006', 'N12905', 'N45076', 'N63273'], ['N59981', 'N13801', 'N32791', 'N8957', 'N154'], ['N3418', 'N14029', 'N976', 'N33831', 'N56214'], ['N13930', 'N55204', 'N19542', 'N43502', 'N38662'], ['N50872', 'N0', 'N0', 'N0', 'N35815'], ['N20069', 'N32544', 'N41020', 'N33619', 'N63970'], ['N55645', 'N44324', 'N41881', 'N3491', 'N3894'], ['N50872', 'N6477', 'N51570', 'N36226', 'N41387'], ['N8373', 'N8595', 'N41934', 'N6837', 'N29212'], ['N57651', 'N36681', 'N0', 'N0', 'N49685'], ['N21753', 'N28495', 'N63970', 'N0', 'N58363'], ['N31763', 'N28413', 'N4912', 'N45076', 'N50314'], ['N646', 'N62360', 'N30475', 'N58363', 'N50675'], ['N3418', 'N18423', 'N26224', 'N40468', 'N19444'], ['N3344', 'N60858', 'N5364', 'N19592', 'N38159'], ['N20041', 'N64748', 'N45016', 'N60105', 'N63154'], ['N35737', 'N63550', 'N55689', 'N55204', 'N4936'], ['N16148', 'N42457', 'N56214', 'N10746', 'N21712'], ['N20041', 'N30475', 'N30920', 'N34956', 'N6379'], ['N38215', 'N4642', 'N13907', 'N39073', 'N46279'], ['N8559', 'N31448', 'N62049', 'N39187', 'N21712'], ['N51478', 'N21741', 'N2350', 'N21509', 'N54489'], ['N37088', 'N41178', 'N39115', 'N59852', 'N24272'], ['N53111', 'N13051', 'N30108', 'N61214', 'N61572'], ['N60992', 'N59385', 'N20076', 'N25444', 'N14523'], ['N62386', 'N33677', 'N33885', 'N45704', 'N1940'], ['N20076', 'N42977', 'N48063', 'N14592', 'N28213'], ['N51255', 'N31978', 'N47817', 'N20678', 'N55606'], ['N37088', 'N16209', 'N58075', 'N18870', 'N32846'], ['N41578', 'N47981', 'N57090', 'N29128', 'N35729'], ['N44616', 'N42028', 'N61074', 'N18708', 'N1034'], ['N50058', 'N38442', 'N59267', 'N46739', 'N1539'], ['N36681', 'N41224', 'N29128', 'N57651', 'N50060'], ['N25722', 'N19318', 'N60577', 'N8349', 'N31978'], ['N49180', 'N41020', 'N3344', 'N20041', 'N58363'], ['N34579', 'N24423', 'N12029', 'N34915', 'N33240'], ['N61623', 'N2823', 'N42961', 'N33964', 'N46370'], ['N4912', 'N15855', 'N49685', 'N64228', 'N53017'], ['N11830', 'N42961', 'N20240', 'N45076', 'N112'], ['N32437', 'N35729', 'N13801', 'N61214', 'N55689'], ['N42803', 'N31679', 'N55943', 'N42860', 'N23446'], ['N59407', 'N35576', 'N27737', 'N54752', 'N62318'], ['N42416', 'N25138', 'N61623', 'N20224', 'N55689'], ['N48017', 'N50872', 'N11817', 'N50592', 'N38779'], ['N22417', 'N34048', 'N35958', 'N4642', 'N36261'], ['N51896', 'N40065', 'N16342', 'N51346', 'N23414'], ['N34600', 'N27737', 'N13056', 'N42670', 'N57733'], ['N26977', 'N349', 'N55645', 'N24272', 'N13423'], ['N55355', 'N52294', 'N28983', 'N154', 'N3841'], ['N57733', 'N60550', 'N47576', 'N61429', 'N36186'], ['N57651', 'N36681', 'N0', 'N0', 'N49685'], ['N33619', 'N2952', 'N40356', 'N3737', 'N7328'], ['N19318', 'N646', 'N50675', 'N1421', 'N35133'], ['N6926', 'N60374', 'N47020', 'N39342', 'N1034'], ['N47061', 'N56214', 'N8509', 'N21420', 'N7618'], ['N30518', 'N46029', 'N46526', 'N41222', 'N53585'], ['N4642', 'N64252', 'N14029', 'N51048', 'N56214'], ['N22257', 'N55275', 'N15574', 'N19019', 'N6816'], ['N7821', 'N57283', 'N26488', 'N34110', 'N64902'], ['N33240', 'N29822', 'N37905', 'N11087', 'N7821'], ['N35773', 'N34504', 'N19050', 'N39949', 'N61949'], ['N61768', 'N61022', 'N54595', 'N41220', 'N1034']]\n",
    " )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
