{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "from utils.util import *\n",
    "from train_test import *\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from parse_config import ConfigParser"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_compressed_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "restored_data = load_compressed_pickle('/datasets/mind_data/data_dict_compressed.pickle')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "user_history=restored_data[\"user_history\"]\n",
    "entity_embedding=restored_data[\"entity_embedding\"]\n",
    "relation_embedding=restored_data[\"relation_embedding\"]\n",
    "entity_adj=restored_data[\"entity_adj\"]\n",
    "relation_adj=restored_data[\"relation_adj\"]\n",
    "news_feature=restored_data[\"news_feature\"]\n",
    "max_entity_freq=restored_data[\"max_entity_freq\"]\n",
    "max_entity_pos=restored_data[\"max_entity_pos\"]\n",
    "max_entity_type=restored_data[\"max_entity_type\"]\n",
    "train_data=restored_data[\"train_data\"]\n",
    "dev_data=restored_data[\"dev_data\"]\n",
    "vert_train=restored_data[\"vert_train\"]\n",
    "vert_test=restored_data[\"vert_test\"]\n",
    "pop_train=restored_data[\"pop_train\"]\n",
    "pop_test=restored_data[\"pop_test\"]\n",
    "item2item_train=restored_data[\"item2item_train\"]\n",
    "item2item_test=restored_data[\"item2item_test\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='KRED')\n",
    "parser.add_argument('-f')\n",
    "parser.add_argument('-c', '--config', default=\"./config.json\", type=str,\n",
    "                    help='config file path (default: None)')\n",
    "parser.add_argument('-r', '--resume', default=None, type=str,\n",
    "                    help='path to latest checkpoint (default: None)')\n",
    "parser.add_argument('-d', '--device', default=None, type=str,\n",
    "                    help='indices of GPUs to enable (default: all)')\n",
    "config = ConfigParser.from_args(parser)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "train_type = \"single_task\"\n",
    "task = \"user2item\"\n",
    "\n",
    "config['trainer']['epochs'] = epochs\n",
    "config['data_loader']['batch_size'] = batch_size\n",
    "config['trainer']['training_type'] = train_type\n",
    "config['trainer']['task'] = task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "if config['trainer']['training_type']  == \"multi-task\":\n",
    "    data = user_history, entity_embedding, relation_embedding, entity_adj, relation_adj, news_feature, max_entity_freq, max_entity_pos, max_entity_type, train_data, dev_data, vert_train, vert_test, pop_train, pop_test, item2item_train, item2item_test\n",
    "elif config['trainer']['task'] == \"user2item\":\n",
    "    data = user_history, entity_embedding, relation_embedding, entity_adj, relation_adj, news_feature, max_entity_freq, max_entity_pos, max_entity_type, train_data, dev_data\n",
    "elif config['trainer']['task'] == \"item2item\":\n",
    "    data =  user_history, entity_embedding, relation_embedding, entity_adj, relation_adj, news_feature, max_entity_freq, max_entity_pos, max_entity_type, item2item_train, item2item_test\n",
    "elif config['trainer']['task'] == \"vert_classify\":\n",
    "    data = user_history, entity_embedding, relation_embedding, entity_adj, relation_adj, news_feature, max_entity_freq, max_entity_pos, max_entity_type, vert_train, vert_test\n",
    "elif config['trainer']['task'] == \"pop_predict\":\n",
    "    data = user_history, entity_embedding, relation_embedding, entity_adj, relation_adj, news_feature, max_entity_freq, max_entity_pos, max_entity_type, pop_train, pop_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#Single Task Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "user_history_dict, entity_embedding, relation_embedding, entity_adj, relation_adj, doc_feature_dict, entity_num, position_num, type_num, train_data, test_data = data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "train_data_u2i = NewsDataset(train_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "train_sampler_u2i = RandomSampler(train_data_u2i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "train_dataloader_u2i = DataLoader(train_data_u2i,\n",
    "                                  sampler=train_sampler_u2i,batch_size=config['data_loader']['batch_size'],collate_fn=my_collate_fn, pin_memory=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "criterion = Softmax_BCELoss(config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "train_data_loader = train_dataloader_u2i"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "device, deviceids = prepare_device(config['n_gpu'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "model = KREDModel(config, user_history_dict, doc_feature_dict, entity_embedding, relation_embedding, entity_adj,relation_adj, entity_num, position_num, type_num).cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config['optimizer']['lr'], weight_decay=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "trainer = Trainer(config, model, criterion, optimizer, device, train_data_loader, data[-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Training epoch 0/6 - 0.0\n",
      "######\n",
      " Step: 0, 0.0 \n",
      "######\n",
      "######\n",
      " Step: 1000, 0.27078256160303277 \n",
      "######\n",
      "######\n",
      " Step: 2000, 0.5415651232060655 \n",
      "######\n",
      "######\n",
      " Step: 3000, 0.8123476848090982 \n",
      "######\n",
      "all loss: tensor(1707.1224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "_train_epoch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [22]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/notebooks/trainer/trainer.py:114\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_epoch(epoch)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_train_epoch\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 114\u001B[0m valid_socre \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_valid_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalid_socre\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    116\u001B[0m valid_scores\u001B[38;5;241m.\u001B[39mappend(valid_socre)\n",
      "File \u001B[0;32m/notebooks/trainer/trainer.py:79\u001B[0m, in \u001B[0;36mTrainer._valid_epoch\u001B[0;34m(self, epoch)\u001B[0m\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m         end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m---> 79\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mitem1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m:\u001B[49m\u001B[43mend\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mitem2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m:\u001B[49m\u001B[43mend\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     80\u001B[0m \u001B[43m                     \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrainer\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtask\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[\n\u001B[1;32m     81\u001B[0m         \u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m     83\u001B[0m     y_pred\u001B[38;5;241m.\u001B[39mextend(out)\n\u001B[1;32m     84\u001B[0m truth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/notebooks/model/KRED.py:65\u001B[0m, in \u001B[0;36mKREDModel.forward\u001B[0;34m(self, user_features, news_features, task)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     64\u001B[0m     user_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muser_modeling(user_features)\n\u001B[0;32m---> 65\u001B[0m     candidate_news_embedding, topk_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnews_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnews_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(candidate_news_embedding\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlen\u001B[39m(user_embedding\u001B[38;5;241m.\u001B[39mshape):\n\u001B[1;32m     67\u001B[0m         user_embedding \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39munsqueeze(user_embedding, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/notebooks/model/News_embedding.py:156\u001B[0m, in \u001B[0;36mNews_embedding.forward\u001B[0;34m(self, news_id)\u001B[0m\n\u001B[1;32m    153\u001B[0m istitle_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_title_embedding(istitle)\n\u001B[1;32m    154\u001B[0m type_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_type_embedding(type_)\n\u001B[0;32m--> 156\u001B[0m kgat_entity_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkgat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mentities\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# batch(news num) * entity num\u001B[39;00m\n\u001B[1;32m    157\u001B[0m news_entity_embedding \u001B[38;5;241m=\u001B[39m kgat_entity_embeddings \u001B[38;5;241m+\u001B[39m entity_num_embedding \u001B[38;5;241m+\u001B[39m istitle_embedding \u001B[38;5;241m+\u001B[39m type_embedding \u001B[38;5;66;03m#todo\u001B[39;00m\n\u001B[1;32m    159\u001B[0m aggregate_embedding, topk_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_layer(news_entity_embedding, context_vecs)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/notebooks/model/KGAT.py:150\u001B[0m, in \u001B[0;36mKGAT.forward\u001B[0;34m(self, entity_ids)\u001B[0m\n\u001B[1;32m    145\u001B[0m neighbor_entities, neighbor_relations \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_neighbors(entity_ids)\n\u001B[1;32m    146\u001B[0m \u001B[38;5;66;03m# Some entity do not have embeddings\u001B[39;00m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;66;03m# print(torch.tensor(entity_ids).max(), 'max1')\u001B[39;00m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;66;03m# print(torch.tensor(neighbor_entities).max(), 'max12')\u001B[39;00m\n\u001B[0;32m--> 150\u001B[0m entity_embedding_lookup \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mEmbedding\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mentity_embedding\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    151\u001B[0m relation_embedding_lookup \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mEmbedding\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelation_embedding\u001B[38;5;241m.\u001B[39mcuda())\n\u001B[1;32m    152\u001B[0m neighbor_entity_embedding \u001B[38;5;241m=\u001B[39m entity_embedding_lookup(torch\u001B[38;5;241m.\u001B[39mtensor(neighbor_entities)\u001B[38;5;241m.\u001B[39mcuda())\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "model323 = trainer.model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "KREDModel(\n  (news_embedding): News_embedding(\n    (kgat): KGAT(\n      (attention_layer1): Linear(in_features=300, out_features=128, bias=True)\n      (attention_layer2): Linear(in_features=128, out_features=1, bias=True)\n      (softmax): Softmax(dim=-1)\n      (relu): ReLU(inplace=True)\n      (convolve_layer): Linear(in_features=200, out_features=100, bias=True)\n    )\n    (final_embedding1): Linear(in_features=868, out_features=128, bias=True)\n    (final_embedding2): Linear(in_features=128, out_features=100, bias=True)\n    (relu): ReLU(inplace=True)\n    (sigmoid): Sigmoid()\n    (tanh): Tanh()\n    (title_embeddings): Embedding(1000, 100)\n    (type_embeddings): Embedding(100, 100)\n    (entity_num_embeddings): Embedding(100, 100)\n    (attention_embedding_layer1): Linear(in_features=868, out_features=128, bias=True)\n    (attention_embedding_layer2): Linear(in_features=128, out_features=1, bias=True)\n    (softmax): Softmax(dim=-2)\n  )\n  (user_modeling): User_modeling(\n    (news_embedding): News_embedding(\n      (kgat): KGAT(\n        (attention_layer1): Linear(in_features=300, out_features=128, bias=True)\n        (attention_layer2): Linear(in_features=128, out_features=1, bias=True)\n        (softmax): Softmax(dim=-1)\n        (relu): ReLU(inplace=True)\n        (convolve_layer): Linear(in_features=200, out_features=100, bias=True)\n      )\n      (final_embedding1): Linear(in_features=868, out_features=128, bias=True)\n      (final_embedding2): Linear(in_features=128, out_features=100, bias=True)\n      (relu): ReLU(inplace=True)\n      (sigmoid): Sigmoid()\n      (tanh): Tanh()\n      (title_embeddings): Embedding(1000, 100)\n      (type_embeddings): Embedding(100, 100)\n      (entity_num_embeddings): Embedding(100, 100)\n      (attention_embedding_layer1): Linear(in_features=868, out_features=128, bias=True)\n      (attention_embedding_layer2): Linear(in_features=128, out_features=1, bias=True)\n      (softmax): Softmax(dim=-2)\n    )\n    (user_attention_layer1): Linear(in_features=100, out_features=128, bias=True)\n    (user_attention_layer2): Linear(in_features=128, out_features=1, bias=True)\n    (relu): ReLU(inplace=True)\n    (softmax): Softmax(dim=0)\n  )\n  (relu): ReLU(inplace=True)\n  (sigmoid): Sigmoid()\n  (softmax): Softmax(dim=-1)\n  (mlp_layer1): Linear(in_features=200, out_features=128, bias=True)\n  (mlp_layer2): Linear(in_features=128, out_features=1, bias=True)\n  (cos): CosineSimilarity()\n  (vert_mlp_layer1): Linear(in_features=100, out_features=128, bias=True)\n  (vert_mlp_layer2): Linear(in_features=128, out_features=15, bias=True)\n  (local_mlp_layer1): Linear(in_features=100, out_features=128, bias=True)\n  (local_mlp_layer2): Linear(in_features=128, out_features=1, bias=True)\n  (pop_mlp_layer1): Linear(in_features=100, out_features=128, bias=True)\n  (pop_mlp_layer2): Linear(in_features=128, out_features=4, bias=True)\n)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model323.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "test_data_u2i = NewsDataset(test_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "test_dataloader_u2i = DataLoader(test_data_u2i,batch_size=config['data_loader']['batch_size'],\n",
    "                                          collate_fn=my_collate_fn, pin_memory=True, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299 ms ± 7.31 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# torch.no_grad() as a context manager:\n",
    "%timeit model323.user_modeling.news_embedding(model323.user_modeling.get_user_history(real_batch_out['item1']))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447 ms ± 5.96 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model323(real_batch_out['item1'], real_batch_out['item2'],config['trainer']['task'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0\n",
      "0\n",
      "100\n",
      "0.00233486656237596\n",
      "-54\n",
      "200\n",
      "0.00466973312475192\n",
      "-109\n",
      "300\n",
      "0.007004599687127881\n",
      "-164\n",
      "400\n",
      "0.00933946624950384\n",
      "-220\n",
      "500\n",
      "0.0116743328118798\n",
      "-274\n",
      "600\n",
      "0.014009199374255762\n",
      "-330\n",
      "700\n",
      "0.01634406593663172\n",
      "-384\n",
      "800\n",
      "0.01867893249900768\n",
      "-438\n",
      "900\n",
      "0.02101379906138364\n",
      "-494\n",
      "1000\n",
      "0.0233486656237596\n",
      "-549\n",
      "1100\n",
      "0.025683532186135564\n",
      "-604\n",
      "1200\n",
      "0.028018398748511524\n",
      "-659\n",
      "1300\n",
      "0.030353265310887483\n",
      "-714\n",
      "1400\n",
      "0.03268813187326344\n",
      "-768\n",
      "1500\n",
      "0.035022998435639406\n",
      "-823\n",
      "1600\n",
      "0.03735786499801536\n",
      "-878\n",
      "1700\n",
      "0.039692731560391326\n",
      "-933\n",
      "1800\n",
      "0.04202759812276728\n",
      "-988\n",
      "1900\n",
      "0.044362464685143245\n",
      "-1043\n",
      "2000\n",
      "0.0466973312475192\n",
      "-1098\n",
      "2100\n",
      "0.049032197809895164\n",
      "-1152\n",
      "2200\n",
      "0.05136706437227113\n",
      "-1207\n",
      "2300\n",
      "0.053701930934647084\n",
      "-1262\n",
      "2400\n",
      "0.05603679749702305\n",
      "-1316\n",
      "2500\n",
      "0.058371664059399\n",
      "-1371\n",
      "2600\n",
      "0.060706530621774966\n",
      "-1426\n",
      "2700\n",
      "0.06304139718415093\n",
      "-1481\n",
      "2800\n",
      "0.06537626374652689\n",
      "-1535\n",
      "2900\n",
      "0.06771113030890284\n",
      "-1590\n",
      "3000\n",
      "0.07004599687127881\n",
      "-1646\n",
      "3100\n",
      "0.07238086343365477\n",
      "-1701\n",
      "3200\n",
      "0.07471572999603072\n",
      "-1756\n",
      "3300\n",
      "0.07705059655840668\n",
      "-1811\n",
      "3400\n",
      "0.07938546312078265\n",
      "-1866\n",
      "3500\n",
      "0.08172032968315861\n",
      "-1922\n",
      "3600\n",
      "0.08405519624553456\n",
      "-1977\n",
      "3700\n",
      "0.08639006280791053\n",
      "-2033\n",
      "3800\n",
      "0.08872492937028649\n",
      "-2087\n",
      "3900\n",
      "0.09105979593266245\n",
      "-2141\n",
      "4000\n",
      "0.0933946624950384\n",
      "-2196\n",
      "4100\n",
      "0.09572952905741437\n",
      "-2251\n",
      "4200\n",
      "0.09806439561979033\n",
      "-2307\n",
      "4300\n",
      "0.10039926218216629\n",
      "-2362\n",
      "4400\n",
      "0.10273412874454226\n",
      "-2417\n",
      "4500\n",
      "0.10506899530691821\n",
      "-2471\n",
      "4600\n",
      "0.10740386186929417\n",
      "-2525\n",
      "4700\n",
      "0.10973872843167012\n",
      "-2580\n",
      "4800\n",
      "0.1120735949940461\n",
      "-2636\n",
      "4900\n",
      "0.11440846155642205\n",
      "-2690\n",
      "5000\n",
      "0.116743328118798\n",
      "-2745\n",
      "5100\n",
      "0.11907819468117398\n",
      "-2800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [42]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     11\u001B[0m batch \u001B[38;5;241m=\u001B[39m real_batch(batch)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m#out = model323(batch['item1'], batch['item2'],config['trainer']['task'])[0].detach().cpu().data.tolist()\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m#y_pred.extend(out)\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m y_pred\u001B[38;5;241m.\u001B[39mextend(\u001B[43mmodel323\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mitem1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mitem2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrainer\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtask\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mtolist())\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/notebooks/model/KRED.py:64\u001B[0m, in \u001B[0;36mKREDModel.forward\u001B[0;34m(self, user_features, news_features, task)\u001B[0m\n\u001B[1;32m     61\u001B[0m         user_embedding \u001B[38;5;241m=\u001B[39m user_embedding\u001B[38;5;241m.\u001B[39mexpand(candidate_news_embedding\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], user_embedding\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m],\n\u001B[1;32m     62\u001B[0m                                            user_embedding\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m])\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 64\u001B[0m     user_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muser_modeling\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m     candidate_news_embedding, topk_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnews_embedding(news_features)\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(candidate_news_embedding\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlen\u001B[39m(user_embedding\u001B[38;5;241m.\u001B[39mshape):\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/notebooks/model/User_modeling.py:42\u001B[0m, in \u001B[0;36mUser_modeling.forward\u001B[0;34m(self, user_id)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, user_id):\n\u001B[1;32m     41\u001B[0m     user_history \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_user_history(user_id)\n\u001B[0;32m---> 42\u001B[0m     user_history_embedding, top_indexs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnews_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser_history\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m     user_attention_modeling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muser_attention_modeling(user_history_embedding)\n\u001B[1;32m     44\u001B[0m     user_embedding \u001B[38;5;241m=\u001B[39m user_attention_modeling\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/notebooks/model/News_embedding.py:156\u001B[0m, in \u001B[0;36mNews_embedding.forward\u001B[0;34m(self, news_id)\u001B[0m\n\u001B[1;32m    153\u001B[0m istitle_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_title_embedding(istitle)\n\u001B[1;32m    154\u001B[0m type_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_type_embedding(type_)\n\u001B[0;32m--> 156\u001B[0m kgat_entity_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkgat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mentities\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# batch(news num) * entity num\u001B[39;00m\n\u001B[1;32m    157\u001B[0m news_entity_embedding \u001B[38;5;241m=\u001B[39m kgat_entity_embeddings \u001B[38;5;241m+\u001B[39m entity_num_embedding \u001B[38;5;241m+\u001B[39m istitle_embedding \u001B[38;5;241m+\u001B[39m type_embedding \u001B[38;5;66;03m#todo\u001B[39;00m\n\u001B[1;32m    159\u001B[0m aggregate_embedding, topk_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_layer(news_entity_embedding, context_vecs)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/notebooks/model/KGAT.py:145\u001B[0m, in \u001B[0;36mKGAT.forward\u001B[0;34m(self, entity_ids)\u001B[0m\n\u001B[1;32m    133\u001B[0m entity_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentity_ids_clearner(entity_ids)\n\u001B[1;32m    134\u001B[0m \u001B[38;5;66;03m# neighbor_entities, neighbor_relations = self.get_neighbors(entity_ids)\u001B[39;00m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;66;03m# Removed not present id\u001B[39;00m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;66;03m# neighbor_entities_tensor = torch.tensor(neighbor_entities).cuda()\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;66;03m# entity_ids_tensor[~filter_mask] = 0\u001B[39;00m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;66;03m# entity_ids_tensor = entity_ids_tensor.cuda()\u001B[39;00m\n\u001B[0;32m--> 145\u001B[0m neighbor_entities, neighbor_relations \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_neighbors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mentity_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;66;03m# Some entity do not have embeddings\u001B[39;00m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;66;03m# print(torch.tensor(entity_ids).max(), 'max1')\u001B[39;00m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;66;03m# print(torch.tensor(neighbor_entities).max(), 'max12')\u001B[39;00m\n\u001B[1;32m    150\u001B[0m entity_embedding_lookup \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mEmbedding\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentity_embedding\u001B[38;5;241m.\u001B[39mcuda())\n",
      "File \u001B[0;32m/notebooks/model/KGAT.py:52\u001B[0m, in \u001B[0;36mKGAT.get_neighbors\u001B[0;34m(self, entities)\u001B[0m\n\u001B[1;32m     50\u001B[0m relation_adj_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madj_relation[entity_i]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m# Some entities that are present in neighbor_entities does not exist in the embedding\u001B[39;00m\n\u001B[0;32m---> 52\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index, (entity_i, relation_i) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mentity_adj_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrelation_adj_list\u001B[49m\u001B[43m)\u001B[49m):\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m entity_i \u001B[38;5;241m>\u001B[39m number_of_entities:\n\u001B[1;32m     54\u001B[0m         entity_adj_list\u001B[38;5;241m.\u001B[39mpop(index)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "y_pred = []\n",
    "time_start = time()\n",
    "for step, batch in enumerate(test_dataloader_u2i):\n",
    "\n",
    "    if (step%100==0):\n",
    "        print(step)\n",
    "        print(step/len(test_dataloader_u2i))\n",
    "        print(int(time_start-time()))\n",
    "    batch = real_batch(batch)\n",
    "    #out = model323(batch['item1'], batch['item2'],config['trainer']['task'])[0].detach().cpu().data.tolist()\n",
    "    #y_pred.extend(out)\n",
    "    y_pred.extend(model323(batch['item1'], batch['item2'],config['trainer']['task'])[0].detach().cpu().data.tolist())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "2740998"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(restored_data[\"dev_data\"]['item1'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "42829"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_list = list(range(0, len(trainer.test_data['label']), int(trainer.config['data_loader']['batch_size'])))\n",
    "len(start_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73152\n"
     ]
    }
   ],
   "source": [
    "fp_dev = open(config['data']['valid_behavior'], 'r', encoding='utf-8')\n",
    "count_lines = 0\n",
    "for lien in fp_dev:\n",
    "    count_lines += 1\n",
    "print(count_lines)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "64\n",
      "128\n",
      "192\n",
      "256\n",
      "320\n",
      "384\n",
      "448\n",
      "512\n",
      "576\n",
      "640\n",
      "704\n",
      "768\n",
      "832\n",
      "896\n",
      "960\n",
      "1024\n",
      "1088\n",
      "1152\n",
      "1216\n",
      "1280\n",
      "1344\n",
      "1408\n",
      "1472\n",
      "1536\n",
      "1600\n",
      "1664\n",
      "1728\n",
      "1792\n",
      "1856\n",
      "1920\n",
      "1984\n",
      "2048\n",
      "2112\n",
      "2176\n",
      "2240\n",
      "2304\n",
      "2368\n",
      "2432\n",
      "2496\n",
      "2560\n",
      "2624\n",
      "2688\n",
      "2752\n",
      "2816\n",
      "2880\n",
      "2944\n",
      "3008\n",
      "3072\n",
      "3136\n",
      "3200\n",
      "3264\n",
      "3328\n",
      "3392\n",
      "3456\n",
      "3520\n",
      "3584\n",
      "3648\n",
      "3712\n",
      "3776\n",
      "3840\n",
      "3904\n",
      "3968\n",
      "4032\n",
      "4096\n",
      "4160\n",
      "4224\n",
      "4288\n",
      "4352\n",
      "4416\n",
      "4480\n",
      "4544\n",
      "4608\n",
      "4672\n",
      "4736\n",
      "4800\n",
      "4864\n",
      "4928\n",
      "4992\n",
      "5056\n",
      "5120\n",
      "5184\n",
      "5248\n",
      "5312\n",
      "5376\n",
      "5440\n",
      "5504\n",
      "5568\n",
      "5632\n",
      "5696\n",
      "5760\n",
      "5824\n",
      "5888\n",
      "5952\n",
      "6016\n",
      "6080\n",
      "6144\n",
      "6208\n",
      "6272\n",
      "6336\n",
      "6400\n",
      "6464\n",
      "6528\n",
      "6592\n",
      "6656\n",
      "6720\n",
      "6784\n",
      "6848\n",
      "6912\n",
      "6976\n",
      "7040\n",
      "7104\n",
      "7168\n",
      "7232\n",
      "7296\n",
      "7360\n",
      "7424\n",
      "7488\n",
      "7552\n",
      "7616\n",
      "7680\n",
      "7744\n",
      "7808\n",
      "7872\n",
      "7936\n",
      "8000\n",
      "8064\n",
      "8128\n",
      "8192\n",
      "8256\n",
      "8320\n",
      "8384\n",
      "8448\n",
      "8512\n",
      "8576\n",
      "8640\n",
      "8704\n",
      "8768\n",
      "8832\n",
      "8896\n",
      "8960\n",
      "9024\n",
      "9088\n",
      "9152\n",
      "9216\n",
      "9280\n",
      "9344\n",
      "9408\n",
      "9472\n",
      "9536\n",
      "9600\n",
      "9664\n",
      "9728\n",
      "9792\n",
      "9856\n",
      "9920\n",
      "9984\n",
      "10048\n",
      "10112\n",
      "10176\n",
      "10240\n",
      "10304\n",
      "10368\n",
      "10432\n",
      "10496\n",
      "10560\n",
      "10624\n",
      "10688\n",
      "10752\n",
      "10816\n",
      "10880\n",
      "10944\n",
      "11008\n",
      "11072\n",
      "11136\n",
      "11200\n",
      "11264\n",
      "11328\n",
      "11392\n",
      "11456\n",
      "11520\n",
      "11584\n",
      "11648\n",
      "11712\n",
      "11776\n",
      "11840\n",
      "11904\n",
      "11968\n",
      "12032\n",
      "12096\n",
      "12160\n",
      "12224\n",
      "12288\n",
      "12352\n",
      "12416\n",
      "12480\n",
      "12544\n",
      "12608\n",
      "12672\n",
      "12736\n",
      "12800\n",
      "12864\n",
      "12928\n",
      "12992\n",
      "13056\n",
      "13120\n",
      "13184\n",
      "13248\n",
      "13312\n",
      "13376\n",
      "13440\n",
      "13504\n",
      "13568\n",
      "13632\n",
      "13696\n",
      "13760\n",
      "13824\n",
      "13888\n",
      "13952\n",
      "14016\n",
      "14080\n",
      "14144\n",
      "14208\n",
      "14272\n",
      "14336\n",
      "14400\n",
      "14464\n",
      "14528\n",
      "14592\n",
      "14656\n",
      "14720\n",
      "14784\n",
      "14848\n",
      "14912\n",
      "14976\n",
      "15040\n",
      "15104\n",
      "15168\n",
      "15232\n",
      "15296\n",
      "15360\n",
      "15424\n",
      "15488\n",
      "15552\n",
      "15616\n",
      "15680\n",
      "15744\n",
      "15808\n",
      "15872\n",
      "15936\n",
      "16000\n",
      "16064\n",
      "16128\n",
      "16192\n",
      "16256\n",
      "16320\n",
      "16384\n",
      "16448\n",
      "16512\n",
      "16576\n",
      "16640\n",
      "16704\n",
      "16768\n",
      "16832\n",
      "16896\n",
      "16960\n",
      "17024\n",
      "17088\n",
      "17152\n",
      "17216\n",
      "17280\n",
      "17344\n",
      "17408\n",
      "17472\n",
      "17536\n",
      "17600\n",
      "17664\n",
      "17728\n",
      "17792\n",
      "17856\n",
      "17920\n",
      "17984\n",
      "18048\n",
      "18112\n",
      "18176\n",
      "18240\n",
      "18304\n",
      "18368\n",
      "18432\n",
      "18496\n",
      "18560\n",
      "18624\n",
      "18688\n",
      "18752\n",
      "18816\n",
      "18880\n",
      "18944\n",
      "19008\n",
      "19072\n",
      "19136\n",
      "19200\n",
      "19264\n",
      "19328\n",
      "19392\n",
      "19456\n",
      "19520\n",
      "19584\n",
      "19648\n",
      "19712\n",
      "19776\n",
      "19840\n",
      "19904\n",
      "19968\n",
      "20032\n",
      "20096\n",
      "20160\n",
      "20224\n",
      "20288\n",
      "20352\n",
      "20416\n",
      "20480\n",
      "20544\n",
      "20608\n",
      "20672\n",
      "20736\n",
      "20800\n",
      "20864\n",
      "20928\n",
      "20992\n",
      "21056\n",
      "21120\n",
      "21184\n",
      "21248\n",
      "21312\n",
      "21376\n",
      "21440\n",
      "21504\n",
      "21568\n",
      "21632\n",
      "21696\n",
      "21760\n",
      "21824\n",
      "21888\n",
      "21952\n",
      "22016\n",
      "22080\n",
      "22144\n",
      "22208\n",
      "22272\n",
      "22336\n",
      "22400\n",
      "22464\n",
      "22528\n",
      "22592\n",
      "22656\n",
      "22720\n",
      "22784\n",
      "22848\n",
      "22912\n",
      "22976\n",
      "23040\n",
      "23104\n",
      "23168\n",
      "23232\n",
      "23296\n",
      "23360\n",
      "23424\n",
      "23488\n",
      "23552\n",
      "23616\n",
      "23680\n",
      "23744\n",
      "23808\n",
      "23872\n",
      "23936\n",
      "24000\n",
      "24064\n",
      "24128\n",
      "24192\n",
      "24256\n",
      "24320\n",
      "24384\n",
      "24448\n",
      "24512\n",
      "24576\n",
      "24640\n",
      "24704\n",
      "24768\n",
      "24832\n",
      "24896\n",
      "24960\n",
      "25024\n",
      "25088\n",
      "25152\n",
      "25216\n",
      "25280\n",
      "25344\n",
      "25408\n",
      "25472\n",
      "25536\n",
      "25600\n",
      "25664\n",
      "25728\n",
      "25792\n",
      "25856\n",
      "25920\n",
      "25984\n",
      "26048\n",
      "26112\n",
      "26176\n",
      "26240\n",
      "26304\n",
      "26368\n",
      "26432\n",
      "26496\n",
      "26560\n",
      "26624\n",
      "26688\n",
      "26752\n",
      "26816\n",
      "26880\n",
      "26944\n",
      "27008\n",
      "27072\n",
      "27136\n",
      "27200\n",
      "27264\n",
      "27328\n",
      "27392\n",
      "27456\n",
      "27520\n",
      "27584\n",
      "27648\n",
      "27712\n",
      "27776\n",
      "27840\n",
      "27904\n",
      "27968\n",
      "28032\n",
      "28096\n",
      "28160\n",
      "28224\n",
      "28288\n",
      "28352\n",
      "28416\n",
      "28480\n",
      "28544\n",
      "28608\n",
      "28672\n",
      "28736\n",
      "28800\n",
      "28864\n",
      "28928\n",
      "28992\n",
      "29056\n",
      "29120\n",
      "29184\n",
      "29248\n",
      "29312\n",
      "29376\n",
      "29440\n",
      "29504\n",
      "29568\n",
      "29632\n",
      "29696\n",
      "29760\n",
      "29824\n",
      "29888\n",
      "29952\n",
      "30016\n",
      "30080\n",
      "30144\n",
      "30208\n",
      "30272\n",
      "30336\n",
      "30400\n",
      "30464\n",
      "30528\n",
      "30592\n",
      "30656\n",
      "30720\n",
      "30784\n",
      "30848\n",
      "30912\n",
      "30976\n",
      "31040\n",
      "31104\n",
      "31168\n",
      "31232\n",
      "31296\n",
      "31360\n",
      "31424\n",
      "31488\n",
      "31552\n",
      "31616\n",
      "31680\n",
      "31744\n",
      "31808\n",
      "31872\n",
      "31936\n",
      "32000\n",
      "32064\n",
      "32128\n",
      "32192\n",
      "32256\n",
      "32320\n",
      "32384\n",
      "32448\n",
      "32512\n",
      "32576\n",
      "32640\n",
      "32704\n",
      "32768\n",
      "32832\n",
      "32896\n",
      "32960\n",
      "33024\n",
      "33088\n",
      "33152\n",
      "33216\n",
      "33280\n",
      "33344\n",
      "33408\n",
      "33472\n",
      "33536\n",
      "33600\n",
      "33664\n",
      "33728\n",
      "33792\n",
      "33856\n",
      "33920\n",
      "33984\n",
      "34048\n",
      "34112\n",
      "34176\n",
      "34240\n",
      "34304\n",
      "34368\n",
      "34432\n",
      "34496\n",
      "34560\n",
      "34624\n",
      "34688\n",
      "34752\n",
      "34816\n",
      "34880\n",
      "34944\n",
      "35008\n",
      "35072\n",
      "35136\n",
      "35200\n",
      "35264\n",
      "35328\n",
      "35392\n",
      "35456\n",
      "35520\n",
      "35584\n",
      "35648\n",
      "35712\n",
      "35776\n",
      "35840\n",
      "35904\n",
      "35968\n",
      "36032\n",
      "36096\n",
      "36160\n",
      "36224\n",
      "36288\n",
      "36352\n",
      "36416\n",
      "36480\n",
      "36544\n",
      "36608\n",
      "36672\n",
      "36736\n",
      "36800\n",
      "36864\n",
      "36928\n",
      "36992\n",
      "37056\n",
      "37120\n",
      "37184\n",
      "37248\n",
      "37312\n",
      "37376\n",
      "37440\n",
      "37504\n",
      "37568\n",
      "37632\n",
      "37696\n",
      "37760\n",
      "37824\n",
      "37888\n",
      "37952\n",
      "38016\n",
      "38080\n",
      "38144\n",
      "38208\n",
      "38272\n",
      "38336\n",
      "38400\n",
      "38464\n",
      "38528\n",
      "38592\n",
      "38656\n",
      "38720\n",
      "38784\n",
      "38848\n",
      "38912\n",
      "38976\n",
      "39040\n",
      "39104\n",
      "39168\n",
      "39232\n",
      "39296\n",
      "39360\n",
      "39424\n",
      "39488\n",
      "39552\n",
      "39616\n",
      "39680\n",
      "39744\n",
      "39808\n",
      "39872\n",
      "39936\n",
      "40000\n",
      "40064\n",
      "40128\n",
      "40192\n",
      "40256\n",
      "40320\n",
      "40384\n",
      "40448\n",
      "40512\n",
      "40576\n",
      "40640\n",
      "40704\n",
      "40768\n",
      "40832\n",
      "40896\n",
      "40960\n",
      "41024\n",
      "41088\n",
      "41152\n",
      "41216\n",
      "41280\n",
      "41344\n",
      "41408\n",
      "41472\n",
      "41536\n",
      "41600\n",
      "41664\n",
      "41728\n",
      "41792\n",
      "41856\n",
      "41920\n",
      "41984\n",
      "42048\n",
      "42112\n",
      "42176\n",
      "42240\n",
      "42304\n",
      "42368\n",
      "42432\n",
      "42496\n",
      "42560\n",
      "42624\n",
      "42688\n",
      "42752\n",
      "42816\n",
      "42880\n",
      "42944\n",
      "43008\n",
      "43072\n",
      "43136\n",
      "43200\n",
      "43264\n",
      "43328\n",
      "43392\n",
      "43456\n",
      "43520\n",
      "43584\n",
      "43648\n",
      "43712\n",
      "43776\n",
      "43840\n",
      "43904\n",
      "43968\n",
      "44032\n",
      "44096\n",
      "44160\n",
      "44224\n",
      "44288\n",
      "44352\n",
      "44416\n",
      "44480\n",
      "44544\n",
      "44608\n",
      "44672\n",
      "44736\n",
      "44800\n",
      "44864\n",
      "44928\n",
      "44992\n",
      "45056\n",
      "45120\n",
      "45184\n",
      "45248\n",
      "45312\n",
      "45376\n",
      "45440\n",
      "45504\n",
      "45568\n",
      "45632\n",
      "45696\n",
      "45760\n",
      "45824\n",
      "45888\n",
      "45952\n",
      "46016\n",
      "46080\n",
      "46144\n",
      "46208\n",
      "46272\n",
      "46336\n",
      "46400\n",
      "46464\n",
      "46528\n",
      "46592\n",
      "46656\n",
      "46720\n",
      "46784\n",
      "46848\n",
      "46912\n",
      "46976\n",
      "47040\n",
      "47104\n",
      "47168\n",
      "47232\n",
      "47296\n",
      "47360\n",
      "47424\n",
      "47488\n",
      "47552\n",
      "47616\n",
      "47680\n",
      "47744\n",
      "47808\n",
      "47872\n",
      "47936\n",
      "48000\n",
      "48064\n",
      "48128\n",
      "48192\n",
      "48256\n",
      "48320\n",
      "48384\n",
      "48448\n",
      "48512\n",
      "48576\n",
      "48640\n",
      "48704\n",
      "48768\n",
      "48832\n",
      "48896\n",
      "48960\n",
      "49024\n",
      "49088\n",
      "49152\n",
      "49216\n",
      "49280\n",
      "49344\n",
      "49408\n",
      "49472\n",
      "49536\n",
      "49600\n",
      "49664\n",
      "49728\n",
      "49792\n",
      "49856\n",
      "49920\n",
      "49984\n",
      "50048\n",
      "50112\n",
      "50176\n",
      "50240\n",
      "50304\n",
      "50368\n",
      "50432\n",
      "50496\n",
      "50560\n",
      "50624\n",
      "50688\n",
      "50752\n",
      "50816\n",
      "50880\n",
      "50944\n",
      "51008\n",
      "51072\n",
      "51136\n",
      "51200\n",
      "51264\n",
      "51328\n",
      "51392\n",
      "51456\n",
      "51520\n",
      "51584\n",
      "51648\n",
      "51712\n",
      "51776\n",
      "51840\n",
      "51904\n",
      "51968\n",
      "52032\n",
      "52096\n",
      "52160\n",
      "52224\n",
      "52288\n",
      "52352\n",
      "52416\n",
      "52480\n",
      "52544\n",
      "52608\n",
      "52672\n",
      "52736\n",
      "52800\n",
      "52864\n",
      "52928\n",
      "52992\n",
      "53056\n",
      "53120\n",
      "53184\n",
      "53248\n",
      "53312\n",
      "53376\n",
      "53440\n",
      "53504\n",
      "53568\n",
      "53632\n",
      "53696\n",
      "53760\n",
      "53824\n",
      "53888\n",
      "53952\n",
      "54016\n",
      "54080\n",
      "54144\n",
      "54208\n",
      "54272\n",
      "54336\n",
      "54400\n",
      "54464\n",
      "54528\n",
      "54592\n",
      "54656\n",
      "54720\n",
      "54784\n",
      "54848\n",
      "54912\n",
      "54976\n",
      "55040\n",
      "55104\n",
      "55168\n",
      "55232\n",
      "55296\n",
      "55360\n",
      "55424\n",
      "55488\n",
      "55552\n",
      "55616\n",
      "55680\n",
      "55744\n",
      "55808\n",
      "55872\n",
      "55936\n",
      "56000\n",
      "56064\n",
      "56128\n",
      "56192\n",
      "56256\n",
      "56320\n",
      "56384\n",
      "56448\n",
      "56512\n",
      "56576\n",
      "56640\n",
      "56704\n",
      "56768\n",
      "56832\n",
      "56896\n",
      "56960\n",
      "57024\n",
      "57088\n",
      "57152\n",
      "57216\n",
      "57280\n",
      "57344\n",
      "57408\n",
      "57472\n",
      "57536\n",
      "57600\n",
      "57664\n",
      "57728\n",
      "57792\n",
      "57856\n",
      "57920\n",
      "57984\n",
      "58048\n",
      "58112\n",
      "58176\n",
      "58240\n",
      "58304\n",
      "58368\n",
      "58432\n",
      "58496\n",
      "58560\n",
      "58624\n",
      "58688\n",
      "58752\n",
      "58816\n",
      "58880\n",
      "58944\n",
      "59008\n",
      "59072\n",
      "59136\n",
      "59200\n",
      "59264\n",
      "59328\n",
      "59392\n",
      "59456\n",
      "59520\n",
      "59584\n",
      "59648\n",
      "59712\n",
      "59776\n",
      "59840\n",
      "59904\n",
      "59968\n",
      "60032\n",
      "60096\n",
      "60160\n",
      "60224\n",
      "60288\n",
      "60352\n",
      "60416\n",
      "60480\n",
      "60544\n",
      "60608\n",
      "60672\n",
      "60736\n",
      "60800\n",
      "60864\n",
      "60928\n",
      "60992\n",
      "61056\n",
      "61120\n",
      "61184\n",
      "61248\n",
      "61312\n",
      "61376\n",
      "61440\n",
      "61504\n",
      "61568\n",
      "61632\n",
      "61696\n",
      "61760\n",
      "61824\n",
      "61888\n",
      "61952\n",
      "62016\n",
      "62080\n",
      "62144\n",
      "62208\n",
      "62272\n",
      "62336\n",
      "62400\n",
      "62464\n",
      "62528\n",
      "62592\n",
      "62656\n",
      "62720\n",
      "62784\n",
      "62848\n",
      "62912\n",
      "62976\n",
      "63040\n",
      "63104\n",
      "63168\n",
      "63232\n",
      "63296\n",
      "63360\n",
      "63424\n",
      "63488\n",
      "63552\n",
      "63616\n",
      "63680\n",
      "63744\n",
      "63808\n",
      "63872\n",
      "63936\n",
      "64000\n",
      "64064\n",
      "64128\n",
      "64192\n",
      "64256\n",
      "64320\n",
      "64384\n",
      "64448\n",
      "64512\n",
      "64576\n",
      "64640\n",
      "64704\n",
      "64768\n",
      "64832\n",
      "64896\n",
      "64960\n",
      "65024\n",
      "65088\n",
      "65152\n",
      "65216\n",
      "65280\n",
      "65344\n",
      "65408\n",
      "65472\n",
      "65536\n",
      "65600\n",
      "65664\n",
      "65728\n",
      "65792\n",
      "65856\n",
      "65920\n",
      "65984\n",
      "66048\n",
      "66112\n",
      "66176\n",
      "66240\n",
      "66304\n",
      "66368\n",
      "66432\n",
      "66496\n",
      "66560\n",
      "66624\n",
      "66688\n",
      "66752\n",
      "66816\n",
      "66880\n",
      "66944\n",
      "67008\n",
      "67072\n",
      "67136\n",
      "67200\n",
      "67264\n",
      "67328\n",
      "67392\n",
      "67456\n",
      "67520\n",
      "67584\n",
      "67648\n",
      "67712\n",
      "67776\n",
      "67840\n",
      "67904\n",
      "67968\n",
      "68032\n",
      "68096\n",
      "68160\n",
      "68224\n",
      "68288\n",
      "68352\n",
      "68416\n",
      "68480\n",
      "68544\n",
      "68608\n",
      "68672\n",
      "68736\n",
      "68800\n",
      "68864\n",
      "68928\n",
      "68992\n",
      "69056\n",
      "69120\n",
      "69184\n",
      "69248\n",
      "69312\n",
      "69376\n",
      "69440\n",
      "69504\n",
      "69568\n",
      "69632\n",
      "69696\n",
      "69760\n",
      "69824\n",
      "69888\n",
      "69952\n",
      "70016\n",
      "70080\n",
      "70144\n",
      "70208\n",
      "70272\n",
      "70336\n",
      "70400\n",
      "70464\n",
      "70528\n",
      "70592\n",
      "70656\n",
      "70720\n",
      "70784\n",
      "70848\n",
      "70912\n",
      "70976\n",
      "71040\n",
      "71104\n",
      "71168\n",
      "71232\n",
      "71296\n",
      "71360\n",
      "71424\n",
      "71488\n",
      "71552\n",
      "71616\n",
      "71680\n",
      "71744\n",
      "71808\n",
      "71872\n",
      "71936\n",
      "72000\n",
      "72064\n",
      "72128\n",
      "72192\n",
      "72256\n",
      "72320\n",
      "72384\n",
      "72448\n",
      "72512\n",
      "72576\n",
      "72640\n",
      "72704\n",
      "72768\n",
      "72832\n",
      "72896\n",
      "72960\n",
      "73024\n",
      "73088\n",
      "73152\n",
      "73216\n",
      "73280\n",
      "73344\n",
      "73408\n",
      "73472\n",
      "73536\n",
      "73600\n",
      "73664\n",
      "73728\n",
      "73792\n",
      "73856\n",
      "73920\n",
      "73984\n",
      "74048\n",
      "74112\n",
      "74176\n",
      "74240\n",
      "74304\n",
      "74368\n",
      "74432\n",
      "74496\n",
      "74560\n",
      "74624\n",
      "74688\n",
      "74752\n",
      "74816\n",
      "74880\n",
      "74944\n",
      "75008\n",
      "75072\n",
      "75136\n",
      "75200\n",
      "75264\n",
      "75328\n",
      "75392\n",
      "75456\n",
      "75520\n",
      "75584\n",
      "75648\n",
      "75712\n",
      "75776\n",
      "75840\n",
      "75904\n",
      "75968\n",
      "76032\n",
      "76096\n",
      "76160\n",
      "76224\n",
      "76288\n",
      "76352\n",
      "76416\n",
      "76480\n",
      "76544\n",
      "76608\n",
      "76672\n",
      "76736\n",
      "76800\n",
      "76864\n",
      "76928\n",
      "76992\n",
      "77056\n",
      "77120\n",
      "77184\n",
      "77248\n",
      "77312\n",
      "77376\n",
      "77440\n",
      "77504\n",
      "77568\n",
      "77632\n",
      "77696\n",
      "77760\n",
      "77824\n",
      "77888\n",
      "77952\n",
      "78016\n",
      "78080\n",
      "78144\n",
      "78208\n",
      "78272\n",
      "78336\n",
      "78400\n",
      "78464\n",
      "78528\n",
      "78592\n",
      "78656\n",
      "78720\n",
      "78784\n",
      "78848\n",
      "78912\n",
      "78976\n",
      "79040\n",
      "79104\n",
      "79168\n",
      "79232\n",
      "79296\n",
      "79360\n",
      "79424\n",
      "79488\n",
      "79552\n",
      "79616\n",
      "79680\n",
      "79744\n",
      "79808\n",
      "79872\n",
      "79936\n",
      "80000\n",
      "80064\n",
      "80128\n",
      "80192\n",
      "80256\n",
      "80320\n",
      "80384\n",
      "80448\n",
      "80512\n",
      "80576\n",
      "80640\n",
      "80704\n",
      "80768\n",
      "80832\n",
      "80896\n",
      "80960\n",
      "81024\n",
      "81088\n",
      "81152\n",
      "81216\n",
      "81280\n",
      "81344\n",
      "81408\n",
      "81472\n",
      "81536\n",
      "81600\n",
      "81664\n",
      "81728\n",
      "81792\n",
      "81856\n",
      "81920\n",
      "81984\n",
      "82048\n",
      "82112\n",
      "82176\n",
      "82240\n",
      "82304\n",
      "82368\n",
      "82432\n",
      "82496\n",
      "82560\n",
      "82624\n",
      "82688\n",
      "82752\n",
      "82816\n",
      "82880\n",
      "82944\n",
      "83008\n",
      "83072\n",
      "83136\n",
      "83200\n",
      "83264\n",
      "83328\n",
      "83392\n",
      "83456\n",
      "83520\n",
      "83584\n",
      "83648\n",
      "83712\n",
      "83776\n",
      "83840\n",
      "83904\n",
      "83968\n",
      "84032\n",
      "84096\n",
      "84160\n",
      "84224\n",
      "84288\n",
      "84352\n",
      "84416\n",
      "84480\n",
      "84544\n",
      "84608\n",
      "84672\n",
      "84736\n",
      "84800\n",
      "84864\n",
      "84928\n",
      "84992\n",
      "85056\n",
      "85120\n",
      "85184\n",
      "85248\n",
      "85312\n",
      "85376\n",
      "85440\n",
      "85504\n",
      "85568\n",
      "85632\n",
      "85696\n",
      "85760\n",
      "85824\n",
      "85888\n",
      "85952\n",
      "86016\n",
      "86080\n",
      "86144\n",
      "86208\n",
      "86272\n",
      "86336\n",
      "86400\n",
      "86464\n",
      "86528\n",
      "86592\n",
      "86656\n",
      "86720\n",
      "86784\n",
      "86848\n",
      "86912\n",
      "86976\n",
      "87040\n",
      "87104\n",
      "87168\n",
      "87232\n",
      "87296\n",
      "87360\n",
      "87424\n",
      "87488\n",
      "87552\n",
      "87616\n",
      "87680\n",
      "87744\n",
      "87808\n",
      "87872\n",
      "87936\n",
      "88000\n",
      "88064\n",
      "88128\n",
      "88192\n",
      "88256\n",
      "88320\n",
      "88384\n",
      "88448\n",
      "88512\n",
      "88576\n",
      "88640\n",
      "88704\n",
      "88768\n",
      "88832\n",
      "88896\n",
      "88960\n",
      "89024\n",
      "89088\n",
      "89152\n",
      "89216\n",
      "89280\n",
      "89344\n",
      "89408\n",
      "89472\n",
      "89536\n",
      "89600\n",
      "89664\n",
      "89728\n",
      "89792\n",
      "89856\n",
      "89920\n",
      "89984\n",
      "90048\n",
      "90112\n",
      "90176\n",
      "90240\n",
      "90304\n",
      "90368\n",
      "90432\n",
      "90496\n",
      "90560\n",
      "90624\n",
      "90688\n",
      "90752\n",
      "90816\n",
      "90880\n",
      "90944\n",
      "91008\n",
      "91072\n",
      "91136\n",
      "91200\n",
      "91264\n",
      "91328\n",
      "91392\n",
      "91456\n",
      "91520\n",
      "91584\n",
      "91648\n",
      "91712\n",
      "91776\n",
      "91840\n",
      "91904\n",
      "91968\n",
      "92032\n",
      "92096\n",
      "92160\n",
      "92224\n",
      "92288\n",
      "92352\n",
      "92416\n",
      "92480\n",
      "92544\n",
      "92608\n",
      "92672\n",
      "92736\n",
      "92800\n",
      "92864\n",
      "92928\n",
      "92992\n",
      "93056\n",
      "93120\n",
      "93184\n",
      "93248\n",
      "93312\n",
      "93376\n",
      "93440\n",
      "93504\n",
      "93568\n",
      "93632\n",
      "93696\n",
      "93760\n",
      "93824\n",
      "93888\n",
      "93952\n",
      "94016\n",
      "94080\n",
      "94144\n",
      "94208\n",
      "94272\n",
      "94336\n",
      "94400\n",
      "94464\n",
      "94528\n",
      "94592\n",
      "94656\n",
      "94720\n",
      "94784\n",
      "94848\n",
      "94912\n",
      "94976\n",
      "95040\n",
      "95104\n",
      "95168\n",
      "95232\n",
      "95296\n",
      "95360\n",
      "95424\n",
      "95488\n",
      "95552\n",
      "95616\n",
      "95680\n",
      "95744\n",
      "95808\n",
      "95872\n",
      "95936\n",
      "96000\n",
      "96064\n",
      "96128\n",
      "96192\n",
      "96256\n",
      "96320\n",
      "96384\n",
      "96448\n",
      "96512\n",
      "96576\n",
      "96640\n",
      "96704\n",
      "96768\n",
      "96832\n",
      "96896\n",
      "96960\n",
      "97024\n",
      "97088\n",
      "97152\n",
      "97216\n",
      "97280\n",
      "97344\n",
      "97408\n",
      "97472\n",
      "97536\n",
      "97600\n",
      "97664\n",
      "97728\n",
      "97792\n",
      "97856\n",
      "97920\n",
      "97984\n",
      "98048\n",
      "98112\n",
      "98176\n",
      "98240\n",
      "98304\n",
      "98368\n",
      "98432\n",
      "98496\n",
      "98560\n",
      "98624\n",
      "98688\n",
      "98752\n",
      "98816\n",
      "98880\n",
      "98944\n",
      "99008\n",
      "99072\n",
      "99136\n",
      "99200\n",
      "99264\n",
      "99328\n",
      "99392\n",
      "99456\n",
      "99520\n",
      "99584\n",
      "99648\n",
      "99712\n",
      "99776\n",
      "99840\n",
      "99904\n",
      "99968\n",
      "100032\n",
      "100096\n",
      "100160\n",
      "100224\n",
      "100288\n",
      "100352\n",
      "100416\n",
      "100480\n",
      "100544\n",
      "100608\n",
      "100672\n",
      "100736\n",
      "100800\n",
      "100864\n",
      "100928\n",
      "100992\n",
      "101056\n",
      "101120\n",
      "101184\n",
      "101248\n",
      "101312\n",
      "101376\n",
      "101440\n",
      "101504\n",
      "101568\n",
      "101632\n",
      "101696\n",
      "101760\n",
      "101824\n",
      "101888\n",
      "101952\n",
      "102016\n",
      "102080\n",
      "102144\n",
      "102208\n",
      "102272\n",
      "102336\n",
      "102400\n",
      "102464\n",
      "102528\n",
      "102592\n",
      "102656\n",
      "102720\n",
      "102784\n",
      "102848\n",
      "102912\n",
      "102976\n",
      "103040\n",
      "103104\n",
      "103168\n",
      "103232\n",
      "103296\n",
      "103360\n",
      "103424\n",
      "103488\n",
      "103552\n",
      "103616\n",
      "103680\n",
      "103744\n",
      "103808\n",
      "103872\n",
      "103936\n",
      "104000\n",
      "104064\n",
      "104128\n",
      "104192\n",
      "104256\n",
      "104320\n",
      "104384\n",
      "104448\n",
      "104512\n",
      "104576\n",
      "104640\n",
      "104704\n",
      "104768\n",
      "104832\n",
      "104896\n",
      "104960\n",
      "105024\n",
      "105088\n",
      "105152\n",
      "105216\n",
      "105280\n",
      "105344\n",
      "105408\n",
      "105472\n",
      "105536\n",
      "105600\n",
      "105664\n",
      "105728\n",
      "105792\n",
      "105856\n",
      "105920\n",
      "105984\n",
      "106048\n",
      "106112\n",
      "106176\n",
      "106240\n",
      "106304\n",
      "106368\n",
      "106432\n",
      "106496\n",
      "106560\n",
      "106624\n",
      "106688\n",
      "106752\n",
      "106816\n",
      "106880\n",
      "106944\n",
      "107008\n",
      "107072\n",
      "107136\n",
      "107200\n",
      "107264\n",
      "107328\n",
      "107392\n",
      "107456\n",
      "107520\n",
      "107584\n",
      "107648\n",
      "107712\n",
      "107776\n",
      "107840\n",
      "107904\n",
      "107968\n",
      "108032\n",
      "108096\n",
      "108160\n",
      "108224\n",
      "108288\n",
      "108352\n",
      "108416\n",
      "108480\n",
      "108544\n",
      "108608\n",
      "108672\n",
      "108736\n",
      "108800\n",
      "108864\n",
      "108928\n",
      "108992\n",
      "109056\n",
      "109120\n",
      "109184\n",
      "109248\n",
      "109312\n",
      "109376\n",
      "109440\n",
      "109504\n",
      "109568\n",
      "109632\n",
      "109696\n",
      "109760\n",
      "109824\n",
      "109888\n",
      "109952\n",
      "110016\n",
      "110080\n",
      "110144\n",
      "110208\n",
      "110272\n",
      "110336\n",
      "110400\n",
      "110464\n",
      "110528\n",
      "110592\n",
      "110656\n",
      "110720\n",
      "110784\n",
      "110848\n",
      "110912\n",
      "110976\n",
      "111040\n",
      "111104\n",
      "111168\n",
      "111232\n",
      "111296\n",
      "111360\n",
      "111424\n",
      "111488\n",
      "111552\n",
      "111616\n",
      "111680\n",
      "111744\n",
      "111808\n",
      "111872\n",
      "111936\n",
      "112000\n",
      "112064\n",
      "112128\n",
      "112192\n",
      "112256\n",
      "112320\n",
      "112384\n",
      "112448\n",
      "112512\n",
      "112576\n",
      "112640\n",
      "112704\n",
      "112768\n",
      "112832\n",
      "112896\n",
      "112960\n",
      "113024\n",
      "113088\n",
      "113152\n",
      "113216\n",
      "113280\n",
      "113344\n",
      "113408\n",
      "113472\n",
      "113536\n",
      "113600\n",
      "113664\n",
      "113728\n",
      "113792\n",
      "113856\n",
      "113920\n",
      "113984\n",
      "114048\n",
      "114112\n",
      "114176\n",
      "114240\n",
      "114304\n",
      "114368\n",
      "114432\n",
      "114496\n",
      "114560\n",
      "114624\n",
      "114688\n",
      "114752\n",
      "114816\n",
      "114880\n",
      "114944\n",
      "115008\n",
      "115072\n",
      "115136\n",
      "115200\n",
      "115264\n",
      "115328\n",
      "115392\n",
      "115456\n",
      "115520\n",
      "115584\n",
      "115648\n",
      "115712\n",
      "115776\n",
      "115840\n",
      "115904\n",
      "115968\n",
      "116032\n",
      "116096\n",
      "116160\n",
      "116224\n",
      "116288\n",
      "116352\n",
      "116416\n",
      "116480\n",
      "116544\n",
      "116608\n",
      "116672\n",
      "116736\n",
      "116800\n",
      "116864\n",
      "116928\n",
      "116992\n",
      "117056\n",
      "117120\n",
      "117184\n",
      "117248\n",
      "117312\n",
      "117376\n",
      "117440\n",
      "117504\n",
      "117568\n",
      "117632\n",
      "117696\n",
      "117760\n",
      "117824\n",
      "117888\n",
      "117952\n",
      "118016\n",
      "118080\n",
      "118144\n",
      "118208\n",
      "118272\n",
      "118336\n",
      "118400\n",
      "118464\n",
      "118528\n",
      "118592\n",
      "118656\n",
      "118720\n",
      "118784\n",
      "118848\n",
      "118912\n",
      "118976\n",
      "119040\n",
      "119104\n",
      "119168\n",
      "119232\n",
      "119296\n",
      "119360\n",
      "119424\n",
      "119488\n",
      "119552\n",
      "119616\n",
      "119680\n",
      "119744\n",
      "119808\n",
      "119872\n",
      "119936\n",
      "120000\n",
      "120064\n",
      "120128\n",
      "120192\n",
      "120256\n",
      "120320\n",
      "120384\n",
      "120448\n",
      "120512\n",
      "120576\n",
      "120640\n",
      "120704\n",
      "120768\n",
      "120832\n",
      "120896\n",
      "120960\n",
      "121024\n",
      "121088\n",
      "121152\n",
      "121216\n",
      "121280\n",
      "121344\n",
      "121408\n",
      "121472\n",
      "121536\n",
      "121600\n",
      "121664\n",
      "121728\n",
      "121792\n",
      "121856\n",
      "121920\n",
      "121984\n",
      "122048\n",
      "122112\n",
      "122176\n",
      "122240\n",
      "122304\n",
      "122368\n",
      "122432\n",
      "122496\n",
      "122560\n",
      "122624\n",
      "122688\n",
      "122752\n",
      "122816\n",
      "122880\n",
      "122944\n",
      "123008\n",
      "123072\n",
      "123136\n",
      "123200\n",
      "123264\n",
      "123328\n",
      "123392\n",
      "123456\n",
      "123520\n",
      "123584\n",
      "123648\n",
      "123712\n",
      "123776\n",
      "123840\n",
      "123904\n",
      "123968\n",
      "124032\n",
      "124096\n",
      "124160\n",
      "124224\n",
      "124288\n",
      "124352\n",
      "124416\n",
      "124480\n",
      "124544\n",
      "124608\n",
      "124672\n",
      "124736\n",
      "124800\n",
      "124864\n",
      "124928\n",
      "124992\n",
      "125056\n",
      "125120\n",
      "125184\n",
      "125248\n",
      "125312\n",
      "125376\n",
      "125440\n",
      "125504\n",
      "125568\n",
      "125632\n",
      "125696\n",
      "125760\n",
      "125824\n",
      "125888\n",
      "125952\n",
      "126016\n",
      "126080\n",
      "126144\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [61]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m      7\u001B[0m         end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(trainer\u001B[38;5;241m.\u001B[39mtest_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m----> 8\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mitem1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m:\u001B[49m\u001B[43mend\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mitem2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m:\u001B[49m\u001B[43mend\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrainer\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtask\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m     11\u001B[0m truth \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mtest_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     12\u001B[0m auc_score \u001B[38;5;241m=\u001B[39m cal_auc(truth, y_pred)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/notebooks/model/KRED.py:63\u001B[0m, in \u001B[0;36mKREDModel.forward\u001B[0;34m(self, user_features, news_features, task)\u001B[0m\n\u001B[1;32m     60\u001B[0m         user_embedding \u001B[38;5;241m=\u001B[39m user_embedding\u001B[38;5;241m.\u001B[39mexpand(candidate_news_embedding\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], user_embedding\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m],\n\u001B[1;32m     61\u001B[0m                                            user_embedding\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m])\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 63\u001B[0m     user_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muser_modeling\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m     candidate_news_embedding, topk_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnews_embedding(news_features)\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(candidate_news_embedding\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlen\u001B[39m(user_embedding\u001B[38;5;241m.\u001B[39mshape):\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/notebooks/model/User_modeling.py:42\u001B[0m, in \u001B[0;36mUser_modeling.forward\u001B[0;34m(self, user_id)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, user_id):\n\u001B[1;32m     41\u001B[0m     user_history \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_user_history(user_id)\n\u001B[0;32m---> 42\u001B[0m     user_history_embedding, top_indexs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnews_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser_history\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m     user_attention_modeling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muser_attention_modeling(user_history_embedding)\n\u001B[1;32m     44\u001B[0m     user_embedding \u001B[38;5;241m=\u001B[39m user_attention_modeling\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/notebooks/model/News_embedding.py:154\u001B[0m, in \u001B[0;36mNews_embedding.forward\u001B[0;34m(self, news_id)\u001B[0m\n\u001B[1;32m    151\u001B[0m istitle_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_title_embedding(istitle)\n\u001B[1;32m    152\u001B[0m type_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_type_embedding(type_)\n\u001B[0;32m--> 154\u001B[0m kgat_entity_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkgat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mentities\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# batch(news num) * entity num\u001B[39;00m\n\u001B[1;32m    155\u001B[0m news_entity_embedding \u001B[38;5;241m=\u001B[39m kgat_entity_embeddings \u001B[38;5;241m+\u001B[39m entity_num_embedding \u001B[38;5;241m+\u001B[39m istitle_embedding \u001B[38;5;241m+\u001B[39m type_embedding \u001B[38;5;66;03m#todo\u001B[39;00m\n\u001B[1;32m    157\u001B[0m aggregate_embedding, topk_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_layer(news_entity_embedding, torch\u001B[38;5;241m.\u001B[39mFloatTensor(context_vecs)\u001B[38;5;241m.\u001B[39mcuda())\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/notebooks/model/KGAT.py:150\u001B[0m, in \u001B[0;36mKGAT.forward\u001B[0;34m(self, entity_ids)\u001B[0m\n\u001B[1;32m    145\u001B[0m neighbor_entities, neighbor_relations \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_neighbors(entity_ids)\n\u001B[1;32m    146\u001B[0m \u001B[38;5;66;03m# Some entity do not have embeddings\u001B[39;00m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;66;03m#print(torch.tensor(entity_ids).max(), 'max1')\u001B[39;00m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;66;03m#print(torch.tensor(neighbor_entities).max(), 'max12')\u001B[39;00m\n\u001B[0;32m--> 150\u001B[0m entity_embedding_lookup \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mEmbedding\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mentity_embedding\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    151\u001B[0m relation_embedding_lookup \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mEmbedding\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelation_embedding\u001B[38;5;241m.\u001B[39mcuda())\n\u001B[1;32m    152\u001B[0m neighbor_entity_embedding \u001B[38;5;241m=\u001B[39m entity_embedding_lookup(torch\u001B[38;5;241m.\u001B[39mtensor(neighbor_entities)\u001B[38;5;241m.\u001B[39mcuda())\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "for start in start_list:\n",
    "    print(start)\n",
    "    if start + int(trainer.config['data_loader']['batch_size']) <= len(trainer.test_data['label']):\n",
    "        end = start + int(trainer.config['data_loader']['batch_size'])\n",
    "    else:\n",
    "        end = len(trainer.test_data['label'])\n",
    "    out = trainer.model(trainer.test_data['item1'][start:end], trainer.test_data['item2'][start:end],\n",
    "                         trainer.config['trainer']['task'])[0].cpu().data.numpy()\n",
    "\n",
    "truth = trainer.test_data['label']\n",
    "auc_score = cal_auc(truth, y_pred)\n",
    "print(\"auc socre: \" + str(auc_score))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def _valid_epoch(epoch):\n",
    "    \"\"\"\n",
    "    Validate after training an epoch\n",
    "    :param epoch: Integer, current training epoch.\n",
    "    :return: A log that contains information about validation\n",
    "    \"\"\"\n",
    "    self.model.eval()\n",
    "    y_pred = []\n",
    "    start_list = list(range(0, len(self.test_data['label']), int(self.config['data_loader']['batch_size'])))\n",
    "    for start in start_list:\n",
    "        if start + int(self.config['data_loader']['batch_size']) <= len(self.test_data['label']):\n",
    "            end = start + int(self.config['data_loader']['batch_size'])\n",
    "        else:\n",
    "            end = len(self.test_data['label'])\n",
    "        out = self.model(self.test_data['item1'][start:end], self.test_data['item2'][start:end],\n",
    "                         self.config['trainer']['task'])[\n",
    "            0].cpu().data.numpy()\n",
    "\n",
    "        y_pred.extend(out)\n",
    "    truth = self.test_data['label']\n",
    "    auc_score = cal_auc(truth, y_pred)  # had to switch input parameters for it to work\n",
    "    print(\"auc socre: \" + str(auc_score))\n",
    "    return auc_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNLP_project.ipynb\t\t    embedding_part.ipynb  parse_config.py\r\n",
      "KG_part.ipynb\t\t\t    framework.PNG\t  req.txt\r\n",
      "README.md\t\t\t    kred_example.ipynb\t  requirements.txt\r\n",
      "__init__.py\t\t\t    logger\t\t  train_test.py\r\n",
      "__pycache__\t\t\t    main.py\t\t  trainer\r\n",
      "base\t\t\t\t    model\t\t  utils\r\n",
      "config.yaml\t\t\t    model_run.ipynb\r\n",
      "data_after_training_singletask.pkl  out\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
